{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Churn Prediction on AWS \u00b6 An End to End customer churn prediction pipeline on AWS. Features \u00b6 One-click Training and Inference Pipelines for churn prediction Preprocessing, Validation, Hyperparameter tuning, and model Explainability all backed into the pipelines Amazon Athena and AWS Glue backend that allows for the pipeline to scale on demand and with new data Reference implementation for your own custom churn pipeline MIT License Getting Started \u00b6 # Set up the resources ./ stand_up . sh AWS_REGION =$ ( aws configure get region ) # Trigger the training pipeline aws lambda -- region $ { AWS_REGION } invoke -- function - name invokeTrainingStepFunction -- payload '{ \"\": \"\"}' out # Trigger the inference pipeline aws lambda -- region $ { AWS_REGION } invoke -- function - name invokeInferStepFunction -- payload '{ \"\": \"\"}' out # Clean up ./ delete_resources . sh Dependencies \u00b6 Before you get started, you will first need an AWS account setup with credentials configured, as explained in this documentation . In addition, make sure the AWS Command Line Interface AWS CLI is already installed. This tutorial assumes that you have an environment with the necessary Identity Access Management IAM permissions . For MAC users the easiest way is to Brew install the AWS CLI: brew install awscli The you can configure credentials with: aws configure For OSes and more info please see the links above. Contributing \u00b6 New features should be discussed first and we also want to prevent that two people are working on the same thing. To get started locally, you can clone the repo and quickly get started using the Makefile . Bugs \u00b6 If you encounter a bug, we'd love to hear about it! We would appreciate though if you could add a reproducible example when you submit an issue on github . We've included some methods to our library to make this relatively easy. Here's an example of a reproducible code-block. FAQ \u00b6 Why this solution? \u00b6 We've seen customers struggling at getting churn setup and running smoothly into production. This framework serves as a good starting point on the technical side.","title":"Index"},{"location":"index.html#churn-prediction-on-aws","text":"An End to End customer churn prediction pipeline on AWS.","title":"Churn Prediction on AWS"},{"location":"index.html#features","text":"One-click Training and Inference Pipelines for churn prediction Preprocessing, Validation, Hyperparameter tuning, and model Explainability all backed into the pipelines Amazon Athena and AWS Glue backend that allows for the pipeline to scale on demand and with new data Reference implementation for your own custom churn pipeline MIT License","title":"Features"},{"location":"index.html#getting-started","text":"# Set up the resources ./ stand_up . sh AWS_REGION =$ ( aws configure get region ) # Trigger the training pipeline aws lambda -- region $ { AWS_REGION } invoke -- function - name invokeTrainingStepFunction -- payload '{ \"\": \"\"}' out # Trigger the inference pipeline aws lambda -- region $ { AWS_REGION } invoke -- function - name invokeInferStepFunction -- payload '{ \"\": \"\"}' out # Clean up ./ delete_resources . sh","title":"Getting Started"},{"location":"index.html#dependencies","text":"Before you get started, you will first need an AWS account setup with credentials configured, as explained in this documentation . In addition, make sure the AWS Command Line Interface AWS CLI is already installed. This tutorial assumes that you have an environment with the necessary Identity Access Management IAM permissions . For MAC users the easiest way is to Brew install the AWS CLI: brew install awscli The you can configure credentials with: aws configure For OSes and more info please see the links above.","title":"Dependencies"},{"location":"index.html#contributing","text":"New features should be discussed first and we also want to prevent that two people are working on the same thing. To get started locally, you can clone the repo and quickly get started using the Makefile .","title":"Contributing"},{"location":"index.html#bugs","text":"If you encounter a bug, we'd love to hear about it! We would appreciate though if you could add a reproducible example when you submit an issue on github . We've included some methods to our library to make this relatively easy. Here's an example of a reproducible code-block.","title":"Bugs"},{"location":"index.html#faq","text":"","title":"FAQ"},{"location":"index.html#why-this-solution","text":"We've seen customers struggling at getting churn setup and running smoothly into production. This framework serves as a good starting point on the technical side.","title":"Why this solution?"},{"location":"inference/batch_transform.html","text":"Inference: Batch Transform \u00b6 Now that the inference dataset is in the proper format you can get churn predictions. This next step make use of Amazon SageMaker\u2019s Batch Transform feature to directly run the inference as a batch job and then writes the data results to S3 into the prefix /data/inference_result .","title":"Batch Inference"},{"location":"inference/batch_transform.html#inference-batch-transform","text":"Now that the inference dataset is in the proper format you can get churn predictions. This next step make use of Amazon SageMaker\u2019s Batch Transform feature to directly run the inference as a batch job and then writes the data results to S3 into the prefix /data/inference_result .","title":"Inference: Batch Transform"},{"location":"inference/preprocessing.html","text":"Inference Preprocessing \u00b6 In this step, you load the saved preprocesser and transform the data so that it\u2019s in the proper format to run churn inference on. Same as the training pipeline this is triggered by the invocation of an Amazon Lambda. aws lambda -- region $ { REGION } invoke -- function - name invokeInferStepFunction -- payload \"{ '': ''}\" out Same as before, job kickoff success is indicated by the 200 code and the inference pipeline starts to run. { \"StatusCode\": 200, \"ExecutedVersion\": \"$LATEST\" } A look in the pipeline.yaml shows that inference data is assumed to be under the table name infer . \"ContainerArguments\": [ \"--database\", \" ${ AthenaDatabaseName } \", \"--region\", \" ${ AWS :: Region } \", \"--table\", \"infer\"], Same as before, the database name {AthendaDatabaseName} is passed in as the name of your stack with -db attached. Region set as the region passed to standup.sh . Likewise, SageMaker configurations are almost completely the same, with the container image still using SageMaker\u2019s Scikit Learn container. The exception here is that instead of scripts/preprocessing.py you will use scripts/inferpreprocessing.py . This script loads the saved training preprocessor from Training Preprocessor to use on the new data. Transformed features are then output back to S3 under the prefix data/intermediate into your designated S3 bucket.","title":"Preprocessing"},{"location":"inference/preprocessing.html#inference-preprocessing","text":"In this step, you load the saved preprocesser and transform the data so that it\u2019s in the proper format to run churn inference on. Same as the training pipeline this is triggered by the invocation of an Amazon Lambda. aws lambda -- region $ { REGION } invoke -- function - name invokeInferStepFunction -- payload \"{ '': ''}\" out Same as before, job kickoff success is indicated by the 200 code and the inference pipeline starts to run. { \"StatusCode\": 200, \"ExecutedVersion\": \"$LATEST\" } A look in the pipeline.yaml shows that inference data is assumed to be under the table name infer . \"ContainerArguments\": [ \"--database\", \" ${ AthenaDatabaseName } \", \"--region\", \" ${ AWS :: Region } \", \"--table\", \"infer\"], Same as before, the database name {AthendaDatabaseName} is passed in as the name of your stack with -db attached. Region set as the region passed to standup.sh . Likewise, SageMaker configurations are almost completely the same, with the container image still using SageMaker\u2019s Scikit Learn container. The exception here is that instead of scripts/preprocessing.py you will use scripts/inferpreprocessing.py . This script loads the saved training preprocessor from Training Preprocessor to use on the new data. Transformed features are then output back to S3 under the prefix data/intermediate into your designated S3 bucket.","title":"Inference Preprocessing"},{"location":"training/hyperparameter_tuning.html","text":"Hyperparameter Tuning \u00b6 The Hyperparameter tuning step finds an optimal combination of hyperparmaters for your model. In this example, you use XGBoost to model churn as a binary outcome (will churn / will not churn). Specifically, you are going to try to achieve the highest accuracy possible through maximizing the Area Under the Curve , finding the best regularization terms, depth and tree splitting combinations in repeated parallel runs (defaulted at 2 total). This produces the most accurate model given the available data. It\u2019s worth noting that there is no script here. All configurations are passed as JSON directly to SageMaker in pipeline.yaml , using SageMaker\u2019s XGBoost container . As before, defaults are hardcoded. However, like all parts of the pipeline, these are updatable as needed. For a deeper look on HyperParameter Tuning with Amazon SageMaker and the the type of inputs possible see here . After this, a Lambda function is called to record the best performing model (and hyperparameter configurations) and then passes it on to the next step in the Step Functions workflow.","title":"HyperParameter Tuning"},{"location":"training/hyperparameter_tuning.html#hyperparameter-tuning","text":"The Hyperparameter tuning step finds an optimal combination of hyperparmaters for your model. In this example, you use XGBoost to model churn as a binary outcome (will churn / will not churn). Specifically, you are going to try to achieve the highest accuracy possible through maximizing the Area Under the Curve , finding the best regularization terms, depth and tree splitting combinations in repeated parallel runs (defaulted at 2 total). This produces the most accurate model given the available data. It\u2019s worth noting that there is no script here. All configurations are passed as JSON directly to SageMaker in pipeline.yaml , using SageMaker\u2019s XGBoost container . As before, defaults are hardcoded. However, like all parts of the pipeline, these are updatable as needed. For a deeper look on HyperParameter Tuning with Amazon SageMaker and the the type of inputs possible see here . After this, a Lambda function is called to record the best performing model (and hyperparameter configurations) and then passes it on to the next step in the Step Functions workflow.","title":"Hyperparameter Tuning"},{"location":"training/model_evaluation.html","text":"Evaluate Model Performance \u00b6 The final step runs a full evaluation on the training and testing data with a report of the results output to S3. The model evaluation step is here as a module because it allows for the customisation of metrics, plots, and hook for different churn use cases. First, the trained model is loaded directly from its stored S3 URI. It then generates a classification report on the testing data and outputs the results as evaluation.json back to S3. Finally, SHAP values and a feature importance plot are output and saved back to S3. Please note, that unlike SageMaker Debugger step in Training Pipeline these outputs are sent directly to your named S3 bucket and not a SageMaker default bucket elsewhere.","title":"Evaluation"},{"location":"training/model_evaluation.html#evaluate-model-performance","text":"The final step runs a full evaluation on the training and testing data with a report of the results output to S3. The model evaluation step is here as a module because it allows for the customisation of metrics, plots, and hook for different churn use cases. First, the trained model is loaded directly from its stored S3 URI. It then generates a classification report on the testing data and outputs the results as evaluation.json back to S3. Finally, SHAP values and a feature importance plot are output and saved back to S3. Please note, that unlike SageMaker Debugger step in Training Pipeline these outputs are sent directly to your named S3 bucket and not a SageMaker default bucket elsewhere.","title":"Evaluate Model Performance"},{"location":"training/model_training.html","text":"Model Training \u00b6 Now with the best model identified, you will re-train one more time to obtain the fully trained model and output model explainability metrics. Again, this step uses SageMaker configurations from pipeline.yaml to run SageMaker\u2019s XGBoost container image. This time around, training is kicking off with optimized hyperparameters and SageMaker Debugger settings. Running the training job with Debugger, allows for explainability metrics to be output to S3 in addition to a fully trained model. Explainability metrics show how each feature affects customer churn. Incorporating techniques like SHAP , enables the ability to explain the model as a whole and, more importantly, the ability to look at how scores are determined on an individual customer basis.","title":"Training"},{"location":"training/model_training.html#model-training","text":"Now with the best model identified, you will re-train one more time to obtain the fully trained model and output model explainability metrics. Again, this step uses SageMaker configurations from pipeline.yaml to run SageMaker\u2019s XGBoost container image. This time around, training is kicking off with optimized hyperparameters and SageMaker Debugger settings. Running the training job with Debugger, allows for explainability metrics to be output to S3 in addition to a fully trained model. Explainability metrics show how each feature affects customer churn. Incorporating techniques like SHAP , enables the ability to explain the model as a whole and, more importantly, the ability to look at how scores are determined on an individual customer basis.","title":"Model Training"},{"location":"training/preporcessing.html","text":"Training Preprocessing Step \u00b6 An Amazon SageMaker preprocessing job is run on data queried directly from the Amazon Athena table making use of SageMaker\u2019s Scikit Learn container. This step is run using scripts/preporcessing.py in the following order: Data is read from the Athena table using awswrangler Data is split into training and validation datasets via a randomized split Missing values are imputed and categorical values are one hot-encoded The split and preprocessed datasets are then written back to S3 as csv files The preprocessor is saved for use in the inference pipeline The preprocessing scripts and Cloud Formation template pipeline.yaml \u2019s Step Functions arguments are update-able. For example, the entry point arguments for the container are set in the CloudFormation as: \"ContainerArguments\": [ \"--database\", \" ${ AthenaDatabaseName } \", \"--region\", \" ${ AWS :: Region } \", \"--table\", \"train\", \"--train-test-split-ratio\", \"0.2\"], The database name {AthendaDatabaseName} is passed in as the name of your stack with -db attached. Region is set from the variable you passed to standup.sh . The table name defaults to the training data name, in this case, \u201ctrain\u201d. Lastly, the random split between train and test is set here as a default, with 25% the data held out for testing. For this blog post, you will leave pipeline.yaml \u2019s settings as is. Keep in mind, it\u2019s possible to change all of these configurations based on your data.","title":"Preprocessing"},{"location":"training/preporcessing.html#training-preprocessing-step","text":"An Amazon SageMaker preprocessing job is run on data queried directly from the Amazon Athena table making use of SageMaker\u2019s Scikit Learn container. This step is run using scripts/preporcessing.py in the following order: Data is read from the Athena table using awswrangler Data is split into training and validation datasets via a randomized split Missing values are imputed and categorical values are one hot-encoded The split and preprocessed datasets are then written back to S3 as csv files The preprocessor is saved for use in the inference pipeline The preprocessing scripts and Cloud Formation template pipeline.yaml \u2019s Step Functions arguments are update-able. For example, the entry point arguments for the container are set in the CloudFormation as: \"ContainerArguments\": [ \"--database\", \" ${ AthenaDatabaseName } \", \"--region\", \" ${ AWS :: Region } \", \"--table\", \"train\", \"--train-test-split-ratio\", \"0.2\"], The database name {AthendaDatabaseName} is passed in as the name of your stack with -db attached. Region is set from the variable you passed to standup.sh . The table name defaults to the training data name, in this case, \u201ctrain\u201d. Lastly, the random split between train and test is set here as a default, with 25% the data held out for testing. For this blog post, you will leave pipeline.yaml \u2019s settings as is. Keep in mind, it\u2019s possible to change all of these configurations based on your data.","title":"Training Preprocessing Step"},{"location":"training/save_model.html","text":"Save the Model \u00b6 This step calls a Lambda function to save the trained model to S3 as an Amazon SageMaker model artifact. The next step retrieves the model URI from S3 to use in the next step for model evaluation.","title":"Model Persistence"},{"location":"training/save_model.html#save-the-model","text":"This step calls a Lambda function to save the trained model to S3 as an Amazon SageMaker model artifact. The next step retrieves the model URI from S3 to use in the next step for model evaluation.","title":"Save the Model"}]}